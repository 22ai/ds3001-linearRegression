{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984d7a11-21b4-4c0c-aa08-948cc7f3b969",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "## Foundations of Machine Learning\n",
    "## `! git clone https://www.github.com/DS3001/linearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d8e09-cd5b-4971-af2a-841f086d49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pn(x):\n",
    "    print(x,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283da74-f7ad-417b-b520-54ced151e4f8",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "- $k$NN and $k$MC illustrate the distinctions between regression and classification, and supervised and unsupervised learning\n",
    "- In both cases, the number of parameters available for fitting the model is really limited --- just $k$ --- and they offer almost no explanation of their results\n",
    "- Today we introduce *linear model*, which optimally weight the explanatory variables in order to predict the outcome variable\n",
    "- These are extremely powerful and easily interpreted tools\n",
    "- You can spend the rest of your life studying regression models (general linear models, quantile regression, kernel regression, etc.): This is an entry-level discussion that focuses on prediction and the intuition/mechanics of Ordinary Least Squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e980e76-2d58-4830-a125-c00f2183be73",
   "metadata": {},
   "source": [
    "## Vector Multiplication\n",
    "- Suppose we have two vectors $x=(x_1,x_2,...x_K)$ and $b=(b_1,b_2,...,b_K)$ of equal length, $K$\n",
    "- The *dot product* or *inner product* is\n",
    "$$\n",
    "x_1 b_1 + x_2 b_2 + ... + x_K b_K\n",
    "$$\n",
    "\n",
    "So we multiply the first two entries together, the second two together, and so on, then sum all the terms.\n",
    "- Common notation for this is:\n",
    "$$\n",
    "x \\cdot b = x^\\top b = x'b = \\langle x, b \\rangle = \\sum_{k=1}^K x_k b_k\n",
    "$$\n",
    "- Notice,\n",
    "$$\\text{cov}(x,y) = \\dfrac{(x-\\bar{x})\\cdot (y-\\bar{y})}{N} = \\dfrac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{N}$$\n",
    "so the covariance is a statistical version of the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd64ef6-c510-427b-a8f4-7b58c38d712f",
   "metadata": {},
   "source": [
    "## Dot Product Example\n",
    "- Suppose `y = (3,-5,7)` and `x = (2,4,-6)`. How do we compute the dot product in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4ad8b-a024-4969-accd-9aeb488d0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = (3,-5,7)\n",
    "x = (2,4,-6)\n",
    "\n",
    "print( np.inner(x,y), '\\n') # Using Numpy\n",
    "\n",
    "def dot(x,y): # Just example code in vanilla Python; avoid loops!\n",
    "    n_x = len(x)\n",
    "    n_y = len(y)\n",
    "    if n_x == n_y:\n",
    "        sum = 0\n",
    "        for k in range(n_x):\n",
    "            sum = sum + x[k]*y[k]\n",
    "        return sum\n",
    "    else:\n",
    "        print('Lengths of arguments are not the same')\n",
    "        raise\n",
    "\n",
    "dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f841e3-60c7-47f6-bffc-f5fdfcbae0df",
   "metadata": {},
   "source": [
    "## Vector Multiplication [math]\n",
    "- What \"is\" this thing? It is related to the angle between $x$ and $b$:\n",
    "$$\n",
    "\\cos(\\theta_{xb}) = \\dfrac{x \\cdot b}{||x|| \\  ||b||}\n",
    "$$\n",
    "The dot product is the mathematical object that creates *angles* between objects in a space, and *covariance* in statistics ($cov(x,y) = (x-\\bar{x})\\cdot(y-\\bar{y})/(N-1)$).\n",
    "- Recall, $\\cos( 90^\\circ) = 0$ or $\\cos(\\pi/2) = 0$, so if the dot product between two vectors is zero, they are at a \"right angle\" to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16c2d8-9fdf-45cf-85bc-d4c0ff9ddb7b",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "- If you stack rows of observations, you can multiply them all at once as follows:\n",
    "$$\n",
    "X \\cdot b =  \\left[\\begin{array}{cccc} x_{11} & x_{12} & \\dots & x_{1K} \\\\ x_{21} & x_{22} & \\dots & x_{2K}  \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{NK} \\end{array} \\right] \\cdot \\left( \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_K \\end{array}\\right) = \\left( \\begin{array}{c} x_1 \\cdot b \\\\ x_2 \\cdot b \\\\ \\vdots \\\\ x_N \\cdot b\\end{array} \\right)\n",
    "$$\n",
    "- This is part of the motivation for \"clean\"/\"tidy\" data: We do calculations directly on the data frame. Having `NA`'s or ambiguity about what a row or column causes calculations to break down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89608e0f-2946-4268-9217-cbe26cb69a3b",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Example\n",
    "- Suppose\n",
    "$$ X = \\left[ \\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{array} \\right] $$\n",
    "and\n",
    "$$ b = \\left( \\begin{array}{c} 2 \\\\ 4 \\\\ 6 \\end{array} \\right)$$\n",
    "- What is $X \\cdot b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67177551-5701-42f2-b87b-5af6104c20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.matrix('1,2,3;4,5,6;7,8,9') # Making a matrix in numpy\n",
    "print('Matrix X:')\n",
    "pn(X)\n",
    "b = np.array([2,4,6]) # Making a vector in numpy\n",
    "print('Vector b:')\n",
    "pn(b)\n",
    "y = np.matmul(X,b) # Matrix multiplication in numpy\n",
    "print('X dot b:')\n",
    "pn(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de37e7-0f4c-47c5-9977-8ce9b79137ae",
   "metadata": {},
   "source": [
    "## Linear Models: Setup\n",
    "- The data include an $N \\times K$ data matrix $X$ with $N$ observations and $K$ variables, and an $N$-length vector of outcomes, $y$\n",
    "- We wish to use $X$ to explain $y$\n",
    "- In particular, we want to explain $y$ by $X$ with a linear model,\n",
    "$$\n",
    "y = X \\cdot b\n",
    "$$\n",
    "where $b$ is a $K$-length vector of *coefficients* or *weights*\n",
    "- So for $b = (b_1, b_2, ..., b_K)$, variable $k$ is multiplied by $b_k$ to scale its contribution to the prediction of $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f570c-921c-4441-8513-825ceaf81023",
   "metadata": {},
   "source": [
    "## Linear Models: Prediction\n",
    "- To make a prediction for a new values $\\hat{x} = (\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_L)$, we compute the dot product:\n",
    "$$\n",
    "\\hat{y} = \\hat{x} \\cdot b = \\hat{x}_1 b_1  + \\hat{x}_2 b_2  + ... + \\hat{x}_K b_K \n",
    "$$\n",
    "- The prediction is the straightforward part: Picking the weights $b$ is the hard part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9466c-74e6-46d5-84b4-71ee078c198e",
   "metadata": {},
   "source": [
    "## Linear Models: Prediction\n",
    "- In terms of matrix multiplication, a fitted model is a $\\hat{b}$, and the predictions are created for data $\\hat{X}$ as\n",
    "$$\n",
    "\\hat{X} \\cdot \\hat{b} =  \\left[\\begin{array}{cccc} \\hat{x}_{11} & \\hat{x}_{12} & \\dots & \\hat{x}_{1K} \\\\ \\hat{x}_{21} & \\hat{x}_{22} & \\dots & \\hat{x}_{2K}  \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat{x}_{N1} & \\hat{x}_{N2} & \\dots & \\hat{x}_{NK} \\end{array} \\right] \\left( \\begin{array}{c} \\hat{b}_1 \\\\ \\hat{b}_2 \\\\ \\vdots \\\\ \\hat{b}_K \\end{array}\\right) = \\left( \\begin{array}{c} \\hat{x}_1 \\cdot \\hat{b} \\\\ \\hat{x}_2 \\cdot \\hat{b} \\\\ \\vdots \\\\ \\hat{x}_N \\cdot \\hat{b}\\end{array} \\right) = \\left( \\begin{array}{c} \\hat{y}_1 \\\\ \\hat{y}_2  \\\\ \\vdots \\\\ \\hat{y}_N \\end{array} \\right)\n",
    "$$\n",
    "- On a compute, these kinds of calulations are fast and efficient, and hardware like GPUs vastly speed up matrix/dot product calculations. Even for very complex models, a linear relationship between variables often appears somewhere (neural networks are non-linear compositions/nests of linear models).\n",
    "- The next chunk of code gives a visual example of what we're talking about, for a simple model $\\hat{y} = b_0 + b_1 x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07654f1b-b460-4afc-b398-568a09995641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(500) # Set the seed for the random number generator\n",
    "N = 30\n",
    "\n",
    "x = 5 + 2*np.random.normal(0,1,size = N) # Create an x\n",
    "eps = np.random.normal(0,3,size = N) # Create noise\n",
    "b0 = -1 # Intercept coefficoent\n",
    "b1 = 3 # Slope coefficient\n",
    "y = b0 + b1*x + eps\n",
    "\n",
    "def slr(x,y): # Single Linear Regression Function\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    b1 = np.inner(x-x_bar,y-y_bar)/np.inner(x-x_bar,x)\n",
    "    b0 = y_bar - b1*x_bar\n",
    "    y_hat = b0 + b1*x\n",
    "    residuals = y - y_hat\n",
    "    return({'b0':b0,'b1':b1,'y_hat':y_hat,'residuals':residuals})\n",
    "    \n",
    "reg = slr(x,y) # Run the regression\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,reg['y_hat'],label='Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43906235-4b16-489c-9708-44102420b202",
   "metadata": {},
   "source": [
    "## Case Study: Car Prices\n",
    "- How does car age predict price?\n",
    "- I'm going to interactively look at the data, clean outliers, take transformations, and regress, somewhat sloppily, then show you what the picture looks like if you don't do these steps\n",
    "- This is a practical set of steps to take a\n",
    "- My rough advice: Linear regressions is defined to approximate the **conditional expectation function** (CEF), $\\mathbb{E}[y|x]$. It is \"working\" if your line is tracking with the average value of $y$ as $x$ varies, and is failing if the line is way off the mark (typically due to 1. outliers or 2. non-linearity of the CEF in $x$, requiring further transformation of the variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104fd15-0c52-4713-91ac-5b1c90cc55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('./data/USA_cars_datasets.csv') # Load the data\n",
    "df0 = df # Let's keep the original data around for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61152a3-2bc3-42a8-a5cb-edc6ae39c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Glance at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a478d4-b9a9-49c7-bcc9-b00319959a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].hist(bins=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a753b8-c08f-497e-abde-c15554ef31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = max(df['year'])-df['year'] # Convert year to age\n",
    "df['age'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff889623-314b-4640-b36c-ec3a39ec4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,y='price',x='age') # We've got some outliers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48850681-19a0-494f-9ee5-52c2db7fd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take arcsinh transformation to rescale the variables\n",
    "df['price_ihs'] = np.arcsinh(df['price'])\n",
    "df['age_ihs'] = np.arcsinh(df['age'])\n",
    "sns.scatterplot(data=df,y='price_ihs',x='age_ihs') # We've got some outliers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a853c-7bbe-4605-b246-99b6b5cee447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_ihs'].plot.box() # Outliers below 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9793e-4046-47bd-81dd-2e4a3f10e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_ihs'].plot.box() # Outliers above 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b73b6-1548-4d77-8c54-9703d99288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers:\n",
    "df = df.loc[df['price_ihs']>9,:]\n",
    "df = df.loc[df['age_ihs']<4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e3770-ff1a-4bb0-af11-7c2efd7be3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,y='price_ihs',x='age_ihs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679189f7-9a77-47bb-89c1-3c830fe626f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['age_ihs']\n",
    "y = df['price_ihs']\n",
    "\n",
    "coef = slr(x,y)\n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression: Looks OK')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe215-2b2f-458f-8f23-3a81aa3ec47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why bother with all that data cleaning?\n",
    "x = df0['age']\n",
    "y = df0['price']\n",
    "\n",
    "coef = slr(x,y)\n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.title('Linear Regression: Looks Bad')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e0705-8e75-43ab-8cf2-c7717a333520",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- Predict car prices from attributes\n",
    "- Predict Airbnb rental prices from housing features\n",
    "- Predict probability of heart failure from patient characteristics\n",
    "- Predict bond and sentence from defendant demographics and criminal record\n",
    "- Predict student loan debt creation as a function of institution characteristics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86037902-98c1-45a7-b4f5-e522eb12af28",
   "metadata": {},
   "source": [
    "## Errors/Residuals\n",
    "- Fitting a linear model is based on minimizing the unexplained variation in the data\n",
    "- Let $\\hat{y}_i = x_i \\cdot b$ be the prediction for observation $i$\n",
    "- The *residual* or *error* for observation $i$ is\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i = y_i - x_i \\cdot b \n",
    "$$\n",
    "This is how far off the in-sample prediction is, using the coefficients $b$ and variables $x_i$ for observation $i$ to make a prediction $\\hat{y}_i$ --- how bad is our model at predicting values for data we already have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f57042-dbdb-44c2-aa6b-706d66271121",
   "metadata": {},
   "source": [
    "## Sum-of-Squared-Error, `SSE`\n",
    "- Some errors will generally be positive and some negative, but we want to count any error as undesirable, and larger errors as even worse failures. So, we square the error,\n",
    "$$\n",
    "e_i^2 = (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "and sum over the observations,\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^N e_i^2 = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "to get the **Sum of Squared Error**. \n",
    "- This is often normalized as an average, to get the **mean squared error**,\n",
    "$$\n",
    "\\text{MSE} = \\dfrac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2,\n",
    "$$\n",
    "and often further normalized by taking the square root to get the **root mean square error**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\dfrac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2}.\n",
    "$$\n",
    "- From a stats perspective there are subtle differences, but from a model-fitting perspective, these are all fundamentally the same thing: A metric of model performance based on squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86784247-2172-48e0-a05e-7b518d5ce428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of the squares of the red lines is the SSE:\n",
    "np.random.seed(500) # Set the seed for the random number generator\n",
    "N = 30\n",
    "x = 5 + 2*np.random.normal(0,1,size = N) # Create an x\n",
    "eps = np.random.normal(0,3,size = N) # Create noise\n",
    "b0 = -1 # Intercept coefficoent\n",
    "b1 = 3 # Slope coefficient\n",
    "y = b0 + b1*x + eps\n",
    "reg = slr(x,y) # Run the regression\n",
    "for i in range(len(x)):\n",
    "    plt.vlines(x[i], y[i], reg['y_hat'][i], color='r') # Visualize residuals\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,reg['y_hat'],label='Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Visualizing SSE: Sum of Squared Red Lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06cd3e-6ee9-41b6-9239-b91429c502d0",
   "metadata": {},
   "source": [
    "## Other Model Metrics\n",
    "- There is no reason you can't target other measures of model performance, like **mean absolute deviation** which is more robust to outliers,\n",
    "$$ \\text{MAD} = \\dfrac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|$$\n",
    "or **worst absolute deviation** where worst-case prediction is the concern,\n",
    "$$ \\text{WAD} = \\max_i |y_i - \\hat{y}_i|$$\n",
    "- There are (at least) hundreds of different metrics of model success besides `SSE`\n",
    "- Why `SSE`? Probably because we can easily use calculus to minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6183b-5a25-437a-91b4-52d58dce2d8b",
   "metadata": {},
   "source": [
    "## Minimizing the `SSE`\n",
    "- The goal of *linear regression* is to pick $b$ to make $\\text{SSE}(b)$ as small as possible. \n",
    "- Let's do this for a *single linear model* with a constant and one explanatory/feature variable $x_i$:\n",
    "$$\n",
    "y_i = b_0 \\times 1 + b_1 \\times x_i\n",
    "$$\n",
    "- Then the `SSE` is:\n",
    "$$\n",
    "\\text{SSE}(b_0, b_1) = \\sum_{i=1}^N (y_i - b_0 - b_1 x_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75755795-3377-42ea-8dd5-508c1501b789",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- From my perpsective, the main reason to study calculus is to learn how to minimize/maximize and approximate functions\n",
    "- To do interesting things in quantitative subjects, you typically need the mathematical background to maximize or minimize things like $\\text{SSE}$ with respsect to $b_0$ and $b_1$\n",
    "- Roughly, to minimize a function $f(b)$, you take the derivative with respect to $b$, $f'(b)$, set it equal to zero, and solve for $b^*$ --- this is called a *first order necessary condition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57bf370-6bea-4cf7-be5c-2bdf3b4fa415",
   "metadata": {},
   "source": [
    "## First-Order Conditions for Optimization\n",
    "- If $\\text{SSE}(b_0,b_1) = \\sum_{i=1}^N(y_i-b_0 - b_1x_i)^2$, the necessary condition for $b_0$ is\n",
    "$$\n",
    "\\sum_{i=1}^N-2(y_i-b_0-b_1 x_i) = 0\n",
    "$$\n",
    "and the necessary condition for $b_1$ is\n",
    "$$\n",
    "\\sum_{i=1}^N -2(y_i - b_0 - b_1 x_i)x_i = 0\n",
    "$$\n",
    "- How do we simplify these?\n",
    "- Define $\\bar{x}$ as the mean of $x$,\n",
    "$$\n",
    "\\bar{x} = \\dfrac{1}{N} \\sum_{i=1}^N x_i,\n",
    "$$\n",
    "and similarly for $\\bar{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb27a0d-d526-4cf3-94c4-5f6b8da62ede",
   "metadata": {},
   "source": [
    "## The First Condition\n",
    "- We can simplify the first condition for $b_0$ as:\n",
    "\\begin{eqnarray*}\n",
    "0 &=& \\sum_{i=1}^N(y_i-b_0-b_1 x_i) \\\\\n",
    "&=&  \\sum_{i=1}^N y_i- \\sum_{i=1}^N b_0-b_1 \\sum_{i=1}^N x_i  \\quad (\\text{Distribute summation})\\\\\n",
    "&=& \\sum_{i=1}^N y_i- N b_0-b_1 \\sum_{i=1}^N x_i \\quad (\\text{Sum $b_0$ $N$ times})\\\\\n",
    "&=& \\dfrac{\\sum_{i=1}^N y_i}{N} -  b_0-b_1 \\dfrac{\\sum_{i=1}^N x_i}{N} \\quad (\\text{Divide by $N$})\\\\\n",
    "0 &=& \\bar{y} - b_0 - b_1 \\bar{x} \\quad (\\text{Use definitions})\n",
    "\\end{eqnarray*}\n",
    "implying $b_0^* = \\bar{y} - b_1^* \\bar{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0312382-89a6-45c0-b633-5c6148c0bfe9",
   "metadata": {},
   "source": [
    "## The Second Condition\n",
    "- For the second condition for $b_1$:\n",
    "\\begin{eqnarray*}\n",
    "0 &=& \\sum_{i=1}^N (y_i - b_0 - b_1 x_i)x_i \\\\\n",
    "&=& \\sum_{i=1}^N (y_i - (\\bar{y} - b_1 \\bar{x}) - b_1 x_i)x_i \\quad (\\text{Substitute in $b_0^*$})\\\\\n",
    "&=& \\sum_{i=1}^N (y_i - \\bar{y})x_i - b_1 (x_i -\\bar{x})x_i \\quad (\\text{Distribute $x_i$, group terms})\\\\\n",
    "0 &=& \\sum_{i=1}^N (y_i - \\bar{y})x_i - b_1 \\sum_{i=1}^N (x_i -\\bar{x})x_i \\quad (\\text{Distribute summation})\\\\\n",
    "\\end{eqnarray*}\n",
    "implying\n",
    "$$\n",
    "b_1^* = \\dfrac{\\sum_{i=1}^N (y_i - \\bar{y})x_i}{\\sum_{i=1}^N (x_i -\\bar{x})x_i}.\n",
    "$$\n",
    "(This is roughly the correlation between $x$ and $y$ divided by the variance of $x$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2212f8e-81b8-4e4b-a25e-c12063dd5b88",
   "metadata": {},
   "source": [
    "## Single Linear Regression \n",
    "- Notice that the first condition can be written as\n",
    "$$\n",
    "0 = \\sum_{i=1}^N (y_i - b_0 - b_1 x_i) \n",
    "  = \\sum_{i=1}^N (y_i - \\hat{y}_i)\n",
    " = \\dfrac{1}{N} \\sum_{i=1}^N e_i \n",
    "$$\n",
    "so **the average error is equal to zero at the optimum**\n",
    "- The second condition can be written as\n",
    "$$\n",
    "0 = \\sum_{i=1}^N (y_i - b_0 - b_1 x_i)x_i \n",
    "= \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i \n",
    " = \\sum_{i=1}^N e_i x_i \n",
    " = e \\cdot x \n",
    "$$\n",
    "**The error term and explanatory variable are statistically uncorrelated, and at \"right angles\" to one another (orthogonal)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209058c-0eba-46dd-9203-618ed742be83",
   "metadata": {},
   "source": [
    "## Single Linear Regression Function\n",
    "- Here is an implementation of single linear regression, which returns a dictionary including the coefficients, the predicted values, and the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d802341-4d04-4b2e-bfc1-b34d1ee22a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slr(x,y): # Single Linear Regression Function\n",
    "    x_bar = np.mean(x) # Average of x's\n",
    "    y_bar = np.mean(y) # Average of y's\n",
    "    b1 = np.inner(x-x_bar,y-y_bar)/np.inner(x-x_bar,x) # Slope coefficient\n",
    "    b0 = y_bar - b1*x_bar # Intercept coefficient\n",
    "    y_hat = b0 + b1*x   # Compute predictions\n",
    "    residuals = y - y_hat   # Compute residuals\n",
    "    return({'b0':b0,'b1':b1,'y_hat':y_hat,'residuals':residuals})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a14730-5a45-4253-a6b3-0adf822baa0f",
   "metadata": {},
   "source": [
    "## Partialing Out, Projection\n",
    "- The regression breaks $y$ into two pieces:\n",
    "\\begin{eqnarray*}\n",
    "y_i &=& (y_i - \\hat{y}_i) + \\hat{y}_i\\\\\n",
    "&=& e_i + \\hat{y}_i\\\\\n",
    "\\underbrace{y_i}_{\\text{True value}} &=& \\underbrace{e_i}_{\\text{Error, residual}} + \\underbrace{x_i \\cdot b}_{\\text{Model, prediction}}\n",
    "\\end{eqnarray*}\n",
    "- But the residual from OLS averages to zero: It is uncorrelated with the prediction\n",
    "- You can understand linear regression as removing the variation in $Y$ that can be explained by $X$ --- The residual contains all of the noise, the predictor $\\hat{b} \\cdot x$ contains all of the signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80dddc-bed9-4599-9449-d486cbd4123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(500) # Set the seed for the random number generator\n",
    "N = 30\n",
    "\n",
    "x = 5 + 2*np.random.normal(0,1,size = N) # Create an x\n",
    "eps = np.random.normal(0,3,size = N) # Create noise\n",
    "b0 = -1 # Intercept coefficoent\n",
    "b1 = 3 # Slope coefficient\n",
    "y = b0 + b1*x + eps\n",
    "\n",
    "def slr(x,y): # Single Linear Regression Function\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    b1 = np.inner(x-x_bar,y-y_bar)/np.inner(x-x_bar,x)\n",
    "    b0 = y_bar - b1*x_bar\n",
    "    y_hat = b0 + b1*x\n",
    "    residuals = y - y_hat\n",
    "    return({'b0':b0,'b1':b1,'y_hat':y_hat,'residuals':residuals})\n",
    "    \n",
    "reg = slr(x,y) # Run the regression\n",
    "print('Coefficients: ',reg['b0'],reg['b1'])\n",
    "\n",
    "# Plot the resuts:\n",
    "for i in range(len(x)):\n",
    "    plt.vlines(x[i], y[i], reg['y_hat'][i], color='r') # Visualize residuals\n",
    "\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,reg['y_hat'],label='Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Linear Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b18e9-1af2-4cfa-bdd5-96140510a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x,reg['y_hat'])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y_hat\")\n",
    "plt.title('Prediction: Signal')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x,reg['residuals'])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"e\")\n",
    "plt.title('Residuals: Noise')\n",
    "\n",
    "pd.DataFrame({'residuals':reg['residuals'],'x':x}).cov() # Compute correlation between e and x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d46167-0b5e-4aff-9747-0a6e52ce8977",
   "metadata": {},
   "source": [
    "## Another SLR Example\n",
    "- Now that we understand SLR a bit better, let's do another case study, of Price versus Mileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b963c2-1888-4407-99b1-adc7a6f4a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0 # Let's start over\n",
    "df['mileage'].describe()\n",
    "df['mileage_ihs'] = np.arcsinh(df['mileage'])\n",
    "df['price_ihs'] = np.arcsinh(df['price'])\n",
    "df['mileage_ihs'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99bc0c1-1624-489b-88fd-4dcca6784abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_ihs'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc228-fa83-4c88-bdf3-5704c816e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,y='price_ihs',x='mileage_ihs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983782c-04a2-4166-9be9-d7c29510cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df,y='price_ihs',x='mileage_ihs',kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfe717-d5a7-4845-bd3c-3327f423cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers:\n",
    "df = df.loc[df['mileage_ihs']>9,:] \n",
    "df = df.loc[df['mileage_ihs']<13,:]\n",
    "df = df.loc[df['price_ihs']>9,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c80f2-b732-4b55-a828-c43f275dd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,x='price_ihs',y='mileage_ihs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c1847-1c1e-423e-b596-4dfad7b9577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['mileage_ihs']\n",
    "y = df['price_ihs']\n",
    "\n",
    "coef = slr(x,y) # Single Linear Regression\n",
    "print('Intercept: ',coef['b0'], '\\n', ' Slope: ', coef['b1']) \n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x # Compute predictions\n",
    "\n",
    "# Scatter plot of fit:\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"Mileage, ihs\")\n",
    "plt.ylabel(\"Price, ihs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fa87b-45a6-475b-b050-f4ca0314c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without cleaning/feature engineering:\n",
    "x = df0['mileage']\n",
    "y = df0['price']\n",
    "\n",
    "coef = slr(x,y)\n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"Mileage\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841f367-6cfa-4b24-8a59-1278ff8dd200",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "- The previous discussion is great: we have some geometic intuition about how linear regression works, its mathematical foundation, and its key properties\n",
    "- Problem: It's only worked out for one regressor/feature/explanatory variable\n",
    "- How do we extend this to multiple variables? i.e., the model\n",
    "$$ y = \\underbrace{b_0}_{\\text{Intercept}} \\times 1 + b_1 \\times  x_1 + b_2 \\times x_2 + ... + b_K \\times x_K $$\n",
    "- We adjust the `SSE` to include all the variables of interest:\n",
    "$$ SSE = \\sum_{i=1}^N (y_i - b_0 - b_1 x_{i1} - b_2 x_{i2} - ... - b_K x_{iK} )^2 = \\sum_{i=1}^N (y_i - X_i \\cdot b)^2$$\n",
    "and maximize over $(b_0, b_1, ..., b_K)$.\n",
    "- We won't go over the details analytically, but minimizing $(y-Xb)'(y-Xb)$ can be done computationally (gradient descent) or using linear algebra (the solution is $b^* = (X'X)^{-1}(X'y)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df0295-459f-497b-9833-5dff8a243b7e",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Example\n",
    "- Before we looked at `age` and `mileage` separately as predictors of price\n",
    "- Now that we have MLR, we can combine then into a single model\n",
    "$$ \\text{price} = b_0 + b_{1} \\times \\text{age} + b_{2}\\times\\text{mileage}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759c896-9023-4db1-9baf-34ce09a5aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-step multilinear regression:\n",
    "def mlr(X,y): # Multiple linear regression, matrix algebra approach\n",
    "    XpX = np.matmul(X.transpose(),X) # Compute X'X\n",
    "    Xpy = np.matmul(X.transpose(),y) # Compute X'y\n",
    "    b = np.linalg.solve(XpX, Xpy) # Solve normal equations\n",
    "    y_hat = np.matmul(X,b) # Compute predictions\n",
    "    residuals = y-y_hat # Compute residuals\n",
    "    SSE =  np.inner(residuals,residuals) # Compute SSE\n",
    "    rsq = 1 - SSE/np.inner( y-np.mean(y),y-np.mean(y)) # Compute Rsq\n",
    "    return({'b':b,'y_hat':y_hat,'residuals':residuals,'rsq':rsq,'SSE':SSE})\n",
    "\n",
    "df['(Intercept)'] = 1\n",
    "X = df.loc[:,['(Intercept)','age_ihs','mileage_ihs'] ]\n",
    "y = df['price_ihs']\n",
    "reg = mlr(X,y)\n",
    "print('MLR coefficients: ', reg['b']) # Same values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fcc0c3-7d22-4b9a-966d-c909ba08cb95",
   "metadata": {},
   "source": [
    "## How does MLR work?\n",
    "- This is the deep idea in regression\n",
    "- Consider the $x_k$ whose coefficient you want to compute:\n",
    "    1. Regress $y$ and $x_k$ on all of the other coefficients, yielding residuals $r_y$ and $r_{x_k}$\n",
    "    2. Now regress $r_y$ on $r_{x_k}$ and a constant\n",
    "- The slope coefficient for the procedure described above is the same as for MLR\n",
    "- What does this mean? Linear regression \"partials out\" all of the variation in $y$ that can be explained by the other features, and the optimal weight $b_k$ reflects only the remaining variation in $y$ that can be explained by $x_k$ alone:\n",
    "$$ \n",
    "b_k = \\dfrac{ \\text{cov}(r_y, r_{x_k})}{\\sigma^2_{r_{x_k}}}\n",
    "$$  \n",
    "- We will return to this idea a lot to explain phenomena related to linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972f8f4-19bb-433c-ba9f-3d01a40511c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do all the steps laboriously:\n",
    "\n",
    "# Pick variables for analysis:\n",
    "y = df['price_ihs']\n",
    "x1 = df['mileage_ihs']\n",
    "x2 = df['age_ihs']\n",
    "\n",
    "reg1_y = slr(x1,y) # Regress y on x1\n",
    "reg1_2 = slr(x1,x2) # Regress x2 on x1\n",
    "y_temp = reg1_y['residuals'] # Extract the residual for y\n",
    "x2_temp = reg1_2['residuals'] # Extract the residual for x2\n",
    "reg_y_x2 = slr(x2_temp,y_temp) # Regress residuals on each other\n",
    "print('Age coefficient: ', reg_y_x2['b1'])\n",
    "\n",
    "reg2_y = slr(x2,y) # Regress y on x2\n",
    "reg2_1 = slr(x2,x1) # Regress x1 on x2\n",
    "y_resid = reg2_y['residuals'] # Extract the residual for y\n",
    "x1_resid = reg2_1['residuals'] # Extract the residual for x1\n",
    "reg_y_x1 = slr(x1_resid,y_resid) # Regress residuals on each other\n",
    "print('Mileage coefficient: ', reg_y_x1['b1'])\n",
    "\n",
    "# Compute intercept:\n",
    "b0 = np.mean(y) - reg_y_x1['b1'] * np.mean(x1) - reg_y_x2['b1']*np.mean(x2)\n",
    "print('Intercept: ', b0, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f48b7-5b46-40ea-9a2a-c3c2ce0854a0",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression \n",
    "- Once you have multiple linear regression, you can build much more complex models of phenomena\n",
    "- We probably want to include $\\text{age}^2$ and $\\text{mileage}^2$ to control for non-linear aging effects:\n",
    "$$ \\text{price} = b_0 + b_{1} \\times \\text{age} + b_{2} \\times\\text{age}^2 + b_{3}\\times\\text{mileage} + b_{4}\\times\\text{mileage}^2 $$\n",
    "- This gives a highly flexible and extensible way of modeling how features predict a target variable; you can only unlock the power of linear regression if you are willing to give it a large feature space to work with\n",
    "- Eventually, we want to discipline this process by using data, not just making up models that run the risk of overfitting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6170483-e56c-4e44-b3ca-a7757f69a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b1d65-c6e4-4409-b2e2-ec6fa6a0fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price_ihs']\n",
    "df['mileage_ihs_sq'] = df['mileage_ihs']**2\n",
    "df['age_ihs_sq'] = df['age_ihs']**2\n",
    "df['(Intercept)'] = 1\n",
    "vars = ['(Intercept)','mileage_ihs','mileage_ihs_sq','age_ihs','age_ihs_sq']\n",
    "X = df.loc[:,vars]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f791305-8d64-4385-88be-b60233bfc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = mlr(X,y) # Run multiple linear regression\n",
    "pn(reg['b']) # Print coefficients \n",
    "reg['residuals'].plot.kde() # Plot residuals\n",
    "pn(reg['rsq']) # R-squared measure of model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e25a47-f01f-4a2c-b29d-b3f6a2aaecf9",
   "metadata": {},
   "source": [
    "## The `sklearn.linear_model` Module\n",
    "- Scikit-Learn has a linear regression object that can be used out-of-the box:\n",
    "    - `from sklearn.linear_model import LinearRegression` to load the linear regression module\n",
    "    - `myRegression = LinearRegression().fit(X, y)` fits \n",
    "- You'll see below that the results very similar to the `mlr` function above: They're using the same NumPy code \"under the hood\" to solve for the OLS coefficients, but with some additional feature engineering/normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc59dc7-3446-49c1-b544-52b819c9596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Import linear regression model\n",
    "\n",
    "vars = ['mileage_ihs','mileage_ihs_sq','age_ihs','age_ihs_sq'] # This is a list of variables to use\n",
    "\n",
    "X = df.loc[:,vars] # Construct data matrix\n",
    "pn(X.head()) # Peek at data\n",
    "\n",
    "reg = LinearRegression().fit(X, y) # Fit the linear model\n",
    "pn(reg.intercept_) # Intercept value\n",
    "pn(reg.coef_) # Regression coefficients\n",
    "pn(reg.score(X, y)) # R squared measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bbadd-fdf0-4c28-b6fb-7573b35b2cec",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- `./data/USA_cars_datasets.csv`\n",
    "- `./data/airbnb_hw.csv`\n",
    "- `./data/heart_failure/heart_failure_clinical_records_dataset.csv`\n",
    "- `./data/pierce_county_house_sales.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60debe9f-de82-4e99-b625-d8418ac8ea12",
   "metadata": {},
   "source": [
    "## Coefficient of Determination, $R^2$\n",
    "- A natural question then is, how much noise is left to explain? How well does the model fit the data?\n",
    "- There is a common statistic used to summarize how predictive $X$ is of $y$, using the OLS coefficients, called the *coefficient of determination* or $R^2$:\n",
    "$$\n",
    "R^2 = 1 - \\dfrac{\\sum_{i=1}^N (y_i - x_i \\cdot \\hat{b})^2}{\\sum_{i=1}^N(y_i-\\bar{y})^2}\n",
    "$$\n",
    "- That numerator is the sum-of-squared-error, evaluated at $\\hat{b}$: How much error remains after trying to explain it using OLS?\n",
    "- That denominator is the error of predicting $y_i$ with just the mean as the predictor, $\\bar{y}$\n",
    "- So the ratio is the reduction in the `SSE` by using the explanatory variables rather than just the sample mean\n",
    "- This is a nice way to evaluate the model, but should not become an end in itself: Adding more variables always raises the $R^2$, but does not necessarily improve out-of-sample prediction\n",
    "- $R^2$ isn't an end in itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb1b63-eca0-4858-9e63-899eed0f7eda",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- To fully leverage the power of regression, you have many options to expand the range of variables that the model can use to explain the variation in the data:\n",
    "    - An **interaction term** is when you take two explanatory variables, say $x_1$ and $x_2$, and multiply them together to get a new explanatory variable, $z = x_1 x_2$, like $\\text{mileage} \\times \\text{age}$\n",
    "    - A **polynomial family** is when you take an explanatory variable, say $x$, and compute its powers $x^2$, $x^3$, ... , $x^K$. (There are many kinds of families besides polynomial.)\n",
    "    - A **logarithmic transformation** or **inverse hyperbolic sine transformation**\n",
    "    - A **maxmin normalization** or **z-score normalization**\n",
    "    - More advanced tools like **principal components analysis** decomposition of the data or **splines** that create highly transformations of variables\n",
    "    - Any combination of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f462b-3081-4fcb-a7ea-bfd0acb5e5f1",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "- Datasets often have dozens or thousands of variables, and we can create many more through transforming and interacting variables. Which variables go into a linear regression?\n",
    "- In linear regression, our model is **underfit** if we use too few variables which cannot capture the complex relationships between the features and target variable, while our model is **overfit** if we are using too many variables which are exploiting too many unique features of the training data\n",
    "- In statistics, it is an assumption that the analyst roughly knows the \"true\" data generating process, and properties of an estimator are derived under that assumption - statistics has few useful answers for how to pick the form of the model itself (e.g. Akaike Information Criterion, Bayesian Information Criterion, Mallows' $C_p$)\n",
    "- In machine learning, we will learn some tools and techniques for model selection in a data-driven way later on in the course based on cross validation (e.g. LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec7a38-eafa-4b80-a5be-7b14b95f7d5b",
   "metadata": {},
   "source": [
    "## What, exactly, makes a model linear?\n",
    "- A model is linear because the coefficients, $b$, enter the prediction as\n",
    "$$\n",
    "\\hat{y} = b \\cdot \\hat{x} = b_1 \\hat{x}_1 + b_2 \\hat{x}_2 + ... b_J \\hat{x}_J\n",
    "$$\n",
    "so that the relationship between $\\hat{y}$ and each $\\hat{x}_j$ is a linear one through $b_j$\n",
    "- Taking non-linear transformations of the $x_j$'s simply gives new variables, like $\\log(x_j)$ or $x_j^2$ -- *Transformations of the regressors don't make the model nonlinear*\n",
    "- Likewise, if there was a coefficient written in a funky nonlinear way, like $\\sqrt{ \\text{asinh}(b_j)}$, you can simply relabel that coefficient as $b_j \\leftarrow \\sqrt{ \\text{asinh}(b_j)}$ and make the model linear again.\n",
    "- There *are* many interesting non-linear models, but they are often derived from a domain-specific theory and require additional work to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7b127-d5a7-43e7-a5a9-ff4b8649aacc",
   "metadata": {},
   "source": [
    "## The Classical Assumptions for OLS [stats]\n",
    "- I often see people write the following: \"The neccessary assumptions for linear regression are (i) linearity, (ii) homoskedasticity, (iii) conditional independence of errors, (iv) normality of errors\"\n",
    "- This generates confusion --- These are the assumptions for *OLS to be the best, linear, unbiased estimator (BLUE) of a hypothetical \"true\" $b_0$ in a finite sample, and the $z$-test to be correctly specified*\n",
    "- We are not trying to estimate $b_0$, we are trying to do something else: find the *best linear predictor* of $y$ using the variables $X$. The above conditions are not necessary for that.\n",
    "- You don't need permission to run OLS and make predictions, you need permission to run OLS and interpret the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda309d-2053-4a2a-97a4-e78d08192fae",
   "metadata": {},
   "source": [
    "## General Linear Models\n",
    "- You can spend the rest of your life studying linear models and their generalizations\n",
    "- In many situations, there are additional restrictions on the environment that cannot always be satisfied by a linear model\n",
    "    - The outcome/target variable might be a binary 0/1 outcome, so we are predicting the probability of something occuring. OLS might predict values less than zero or greater than one. Popular solutions are **Logit regression** or **Probit regression**\n",
    "    - The outcome/target variable might be a non-negative integer, meaning that it is **count data**, like the number of earthquakes or cases of an illness. OLS won't predict counts. Popular solutions are **Poisson regression** or **negative binomial regression**.\n",
    "- In these cases, we can estimate alternative models that impose restrictions on the outcome of the linear model, typically through maximum likelihood estimation or generalized linear modeling\n",
    "- These aren't typically much more work to estimate, but we don't really have the time to cover all the nuances (and people typically estimate them only if the OLS model really breaks down in terms of predictive performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
